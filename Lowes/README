README
Lowes
Created by Noah Christiano on 7/21/2014.
noahchristiano@rochester.edu

correct header, no personal information

Lowes scrapes data off of websites which provide page by page search results.

Directory Name		File Name			Content
~					README				Read me
					database_builder.py	Constructs/populates database
					listing.py			Listing object
					lowes_automator.py	Scrapes the Lowes website
					multithreading.py	Script for multithreading
					store_numbers.txt	ID numbers for all the stores
					test.py				Heart of the program
json				json_multi.py		Processing json with multi threading
					json_pool.py		Processing json with multi processing
					json_thread.py		Processing json with threading
					lowes_json.py		Processing json data
locations			collector.py		Process information from Scrapy
					scrapy.cfg			Scrapy configuration
					store.py			Store object
"/locations			__init__.py			Init for scrapy
					items.py			Items object
					pipelines.py		Scrapy pipeline
					settings.py			Scrapy settings
"/spiders			allstays.py			Script for scraping allstays.com with Scrapy
					__init__.py			Init for scrapy spiders

Lowes attempts to create a SQLite database of information about every Pella product at every Lowes location. Obviously, this program has a very narrow application. However by making this code available, I hope that others can use this as a means to accomplish similar tasks.

This project utilizes the Scrapy, Selenium, Requests, JSON, Multiprocessing, and Sqlite3 libraries for Python.

Here is the path that Lowes takes:
- populate list of store identification numbers
- get all Pella products from each store
- enter products in a database

Lowes is more of a project than a program. I have left some of the unused components in the code for those who may find them useful. For example, I initially used Scrapy to populate the list of Lowes store identification numbers (locations). Later, I found that this list was incomplete and used JSON data returned from the "Store Locater" on the Lowes website to create an up-to-date list (lowes_json.py). This proved significantly more efficient as it did not use a virtual browser (Scrapy).

After acquiring my list of ID numbers, I used Selenium to automate my browser to collect the individual Pella products (lowes_automator.py).

There are over 1000 Lowes locations and around 700 Pella products in each store. This scale results in a project too large for my implementation. In an attempt to more fully utilize my machine, I attempted to use multi threading, multiprocessing, and pools to allocate more resources to my task. I also attempted to parallelize the entry of data into the database with the collection of search results by Selenium. My various attempts at this can be found throughout the project in the files with titles containing multi, pool, and thread. I noticed little measurable improvement in performance, which I blame on Python, my implementation, and my machine in that order.
